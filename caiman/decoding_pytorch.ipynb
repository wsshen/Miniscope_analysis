{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('/home/watson/Downloads/data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "['je n en suis pas trop convaincu', 'i m not too convinced']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "    print(input_ids.shape,target_ids.shape)\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for ind,data in enumerate(dataloader):\n",
    "        input_tensor, target_tensor = data\n",
    "        print(ind,input_tensor.shape)\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "(11445, 10) (11445, 10)\n",
      "0 torch.Size([32, 10])\n",
      "1 torch.Size([32, 10])\n",
      "2 torch.Size([32, 10])\n",
      "3 torch.Size([32, 10])\n",
      "4 torch.Size([32, 10])\n",
      "5 torch.Size([32, 10])\n",
      "6 torch.Size([32, 10])\n",
      "7 torch.Size([32, 10])\n",
      "8 torch.Size([32, 10])\n",
      "9 torch.Size([32, 10])\n",
      "10 torch.Size([32, 10])\n",
      "11 torch.Size([32, 10])\n",
      "12 torch.Size([32, 10])\n",
      "13 torch.Size([32, 10])\n",
      "14 torch.Size([32, 10])\n",
      "15 torch.Size([32, 10])\n",
      "16 torch.Size([32, 10])\n",
      "17 torch.Size([32, 10])\n",
      "18 torch.Size([32, 10])\n",
      "19 torch.Size([32, 10])\n",
      "20 torch.Size([32, 10])\n",
      "21 torch.Size([32, 10])\n",
      "22 torch.Size([32, 10])\n",
      "23 torch.Size([32, 10])\n",
      "24 torch.Size([32, 10])\n",
      "25 torch.Size([32, 10])\n",
      "26 torch.Size([32, 10])\n",
      "27 torch.Size([32, 10])\n",
      "28 torch.Size([32, 10])\n",
      "29 torch.Size([32, 10])\n",
      "30 torch.Size([32, 10])\n",
      "31 torch.Size([32, 10])\n",
      "32 torch.Size([32, 10])\n",
      "33 torch.Size([32, 10])\n",
      "34 torch.Size([32, 10])\n",
      "35 torch.Size([32, 10])\n",
      "36 torch.Size([32, 10])\n",
      "37 torch.Size([32, 10])\n",
      "38 torch.Size([32, 10])\n",
      "39 torch.Size([32, 10])\n",
      "40 torch.Size([32, 10])\n",
      "41 torch.Size([32, 10])\n",
      "42 torch.Size([32, 10])\n",
      "43 torch.Size([32, 10])\n",
      "44 torch.Size([32, 10])\n",
      "45 torch.Size([32, 10])\n",
      "46 torch.Size([32, 10])\n",
      "47 torch.Size([32, 10])\n",
      "48 torch.Size([32, 10])\n",
      "49 torch.Size([32, 10])\n",
      "50 torch.Size([32, 10])\n",
      "51 torch.Size([32, 10])\n",
      "52 torch.Size([32, 10])\n",
      "53 torch.Size([32, 10])\n",
      "54 torch.Size([32, 10])\n",
      "55 torch.Size([32, 10])\n",
      "56 torch.Size([32, 10])\n",
      "57 torch.Size([32, 10])\n",
      "58 torch.Size([32, 10])\n",
      "59 torch.Size([32, 10])\n",
      "60 torch.Size([32, 10])\n",
      "61 torch.Size([32, 10])\n",
      "62 torch.Size([32, 10])\n",
      "63 torch.Size([32, 10])\n",
      "64 torch.Size([32, 10])\n",
      "65 torch.Size([32, 10])\n",
      "66 torch.Size([32, 10])\n",
      "67 torch.Size([32, 10])\n",
      "68 torch.Size([32, 10])\n",
      "69 torch.Size([32, 10])\n",
      "70 torch.Size([32, 10])\n",
      "71 torch.Size([32, 10])\n",
      "72 torch.Size([32, 10])\n",
      "73 torch.Size([32, 10])\n",
      "74 torch.Size([32, 10])\n",
      "75 torch.Size([32, 10])\n",
      "76 torch.Size([32, 10])\n",
      "77 torch.Size([32, 10])\n",
      "78 torch.Size([32, 10])\n",
      "79 torch.Size([32, 10])\n",
      "80 torch.Size([32, 10])\n",
      "81 torch.Size([32, 10])\n",
      "82 torch.Size([32, 10])\n",
      "83 torch.Size([32, 10])\n",
      "84 torch.Size([32, 10])\n",
      "85 torch.Size([32, 10])\n",
      "86 torch.Size([32, 10])\n",
      "87 torch.Size([32, 10])\n",
      "88 torch.Size([32, 10])\n",
      "89 torch.Size([32, 10])\n",
      "90 torch.Size([32, 10])\n",
      "91 torch.Size([32, 10])\n",
      "92 torch.Size([32, 10])\n",
      "93 torch.Size([32, 10])\n",
      "94 torch.Size([32, 10])\n",
      "95 torch.Size([32, 10])\n",
      "96 torch.Size([32, 10])\n",
      "97 torch.Size([32, 10])\n",
      "98 torch.Size([32, 10])\n",
      "99 torch.Size([32, 10])\n",
      "100 torch.Size([32, 10])\n",
      "101 torch.Size([32, 10])\n",
      "102 torch.Size([32, 10])\n",
      "103 torch.Size([32, 10])\n",
      "104 torch.Size([32, 10])\n",
      "105 torch.Size([32, 10])\n",
      "106 torch.Size([32, 10])\n",
      "107 torch.Size([32, 10])\n",
      "108 torch.Size([32, 10])\n",
      "109 torch.Size([32, 10])\n",
      "110 torch.Size([32, 10])\n",
      "111 torch.Size([32, 10])\n",
      "112 torch.Size([32, 10])\n",
      "113 torch.Size([32, 10])\n",
      "114 torch.Size([32, 10])\n",
      "115 torch.Size([32, 10])\n",
      "116 torch.Size([32, 10])\n",
      "117 torch.Size([32, 10])\n",
      "118 torch.Size([32, 10])\n",
      "119 torch.Size([32, 10])\n",
      "120 torch.Size([32, 10])\n",
      "121 torch.Size([32, 10])\n",
      "122 torch.Size([32, 10])\n",
      "123 torch.Size([32, 10])\n",
      "124 torch.Size([32, 10])\n",
      "125 torch.Size([32, 10])\n",
      "126 torch.Size([32, 10])\n",
      "127 torch.Size([32, 10])\n",
      "128 torch.Size([32, 10])\n",
      "129 torch.Size([32, 10])\n",
      "130 torch.Size([32, 10])\n",
      "131 torch.Size([32, 10])\n",
      "132 torch.Size([32, 10])\n",
      "133 torch.Size([32, 10])\n",
      "134 torch.Size([32, 10])\n",
      "135 torch.Size([32, 10])\n",
      "136 torch.Size([32, 10])\n",
      "137 torch.Size([32, 10])\n",
      "138 torch.Size([32, 10])\n",
      "139 torch.Size([32, 10])\n",
      "140 torch.Size([32, 10])\n",
      "141 torch.Size([32, 10])\n",
      "142 torch.Size([32, 10])\n",
      "143 torch.Size([32, 10])\n",
      "144 torch.Size([32, 10])\n",
      "145 torch.Size([32, 10])\n",
      "146 torch.Size([32, 10])\n",
      "147 torch.Size([32, 10])\n",
      "148 torch.Size([32, 10])\n",
      "149 torch.Size([32, 10])\n",
      "150 torch.Size([32, 10])\n",
      "151 torch.Size([32, 10])\n",
      "152 torch.Size([32, 10])\n",
      "153 torch.Size([32, 10])\n",
      "154 torch.Size([32, 10])\n",
      "155 torch.Size([32, 10])\n",
      "156 torch.Size([32, 10])\n",
      "157 torch.Size([32, 10])\n",
      "158 torch.Size([32, 10])\n",
      "159 torch.Size([32, 10])\n",
      "160 torch.Size([32, 10])\n",
      "161 torch.Size([32, 10])\n",
      "162 torch.Size([32, 10])\n",
      "163 torch.Size([32, 10])\n",
      "164 torch.Size([32, 10])\n",
      "165 torch.Size([32, 10])\n",
      "166 torch.Size([32, 10])\n",
      "167 torch.Size([32, 10])\n",
      "168 torch.Size([32, 10])\n",
      "169 torch.Size([32, 10])\n",
      "170 torch.Size([32, 10])\n",
      "171 torch.Size([32, 10])\n",
      "172 torch.Size([32, 10])\n",
      "173 torch.Size([32, 10])\n",
      "174 torch.Size([32, 10])\n",
      "175 torch.Size([32, 10])\n",
      "176 torch.Size([32, 10])\n",
      "177 torch.Size([32, 10])\n",
      "178 torch.Size([32, 10])\n",
      "179 torch.Size([32, 10])\n",
      "180 torch.Size([32, 10])\n",
      "181 torch.Size([32, 10])\n",
      "182 torch.Size([32, 10])\n",
      "183 torch.Size([32, 10])\n",
      "184 torch.Size([32, 10])\n",
      "185 torch.Size([32, 10])\n",
      "186 torch.Size([32, 10])\n",
      "187 torch.Size([32, 10])\n",
      "188 torch.Size([32, 10])\n",
      "189 torch.Size([32, 10])\n",
      "190 torch.Size([32, 10])\n",
      "191 torch.Size([32, 10])\n",
      "192 torch.Size([32, 10])\n",
      "193 torch.Size([32, 10])\n",
      "194 torch.Size([32, 10])\n",
      "195 torch.Size([32, 10])\n",
      "196 torch.Size([32, 10])\n",
      "197 torch.Size([32, 10])\n",
      "198 torch.Size([32, 10])\n",
      "199 torch.Size([32, 10])\n",
      "200 torch.Size([32, 10])\n",
      "201 torch.Size([32, 10])\n",
      "202 torch.Size([32, 10])\n",
      "203 torch.Size([32, 10])\n",
      "204 torch.Size([32, 10])\n",
      "205 torch.Size([32, 10])\n",
      "206 torch.Size([32, 10])\n",
      "207 torch.Size([32, 10])\n",
      "208 torch.Size([32, 10])\n",
      "209 torch.Size([32, 10])\n",
      "210 torch.Size([32, 10])\n",
      "211 torch.Size([32, 10])\n",
      "212 torch.Size([32, 10])\n",
      "213 torch.Size([32, 10])\n",
      "214 torch.Size([32, 10])\n",
      "215 torch.Size([32, 10])\n",
      "216 torch.Size([32, 10])\n",
      "217 torch.Size([32, 10])\n",
      "218 torch.Size([32, 10])\n",
      "219 torch.Size([32, 10])\n",
      "220 torch.Size([32, 10])\n",
      "221 torch.Size([32, 10])\n",
      "222 torch.Size([32, 10])\n",
      "223 torch.Size([32, 10])\n",
      "224 torch.Size([32, 10])\n",
      "225 torch.Size([32, 10])\n",
      "226 torch.Size([32, 10])\n",
      "227 torch.Size([32, 10])\n",
      "228 torch.Size([32, 10])\n",
      "229 torch.Size([32, 10])\n",
      "230 torch.Size([32, 10])\n",
      "231 torch.Size([32, 10])\n",
      "232 torch.Size([32, 10])\n",
      "233 torch.Size([32, 10])\n",
      "234 torch.Size([32, 10])\n",
      "235 torch.Size([32, 10])\n",
      "236 torch.Size([32, 10])\n",
      "237 torch.Size([32, 10])\n",
      "238 torch.Size([32, 10])\n",
      "239 torch.Size([32, 10])\n",
      "240 torch.Size([32, 10])\n",
      "241 torch.Size([32, 10])\n",
      "242 torch.Size([32, 10])\n",
      "243 torch.Size([32, 10])\n",
      "244 torch.Size([32, 10])\n",
      "245 torch.Size([32, 10])\n",
      "246 torch.Size([32, 10])\n",
      "247 torch.Size([32, 10])\n",
      "248 torch.Size([32, 10])\n",
      "249 torch.Size([32, 10])\n",
      "250 torch.Size([32, 10])\n",
      "251 torch.Size([32, 10])\n",
      "252 torch.Size([32, 10])\n",
      "253 torch.Size([32, 10])\n",
      "254 torch.Size([32, 10])\n",
      "255 torch.Size([32, 10])\n",
      "256 torch.Size([32, 10])\n",
      "257 torch.Size([32, 10])\n",
      "258 torch.Size([32, 10])\n",
      "259 torch.Size([32, 10])\n",
      "260 torch.Size([32, 10])\n",
      "261 torch.Size([32, 10])\n",
      "262 torch.Size([32, 10])\n",
      "263 torch.Size([32, 10])\n",
      "264 torch.Size([32, 10])\n",
      "265 torch.Size([32, 10])\n",
      "266 torch.Size([32, 10])\n",
      "267 torch.Size([32, 10])\n",
      "268 torch.Size([32, 10])\n",
      "269 torch.Size([32, 10])\n",
      "270 torch.Size([32, 10])\n",
      "271 torch.Size([32, 10])\n",
      "272 torch.Size([32, 10])\n",
      "273 torch.Size([32, 10])\n",
      "274 torch.Size([32, 10])\n",
      "275 torch.Size([32, 10])\n",
      "276 torch.Size([32, 10])\n",
      "277 torch.Size([32, 10])\n",
      "278 torch.Size([32, 10])\n",
      "279 torch.Size([32, 10])\n",
      "280 torch.Size([32, 10])\n",
      "281 torch.Size([32, 10])\n",
      "282 torch.Size([32, 10])\n",
      "283 torch.Size([32, 10])\n",
      "284 torch.Size([32, 10])\n",
      "285 torch.Size([32, 10])\n",
      "286 torch.Size([32, 10])\n",
      "287 torch.Size([32, 10])\n",
      "288 torch.Size([32, 10])\n",
      "289 torch.Size([32, 10])\n",
      "290 torch.Size([32, 10])\n",
      "291 torch.Size([32, 10])\n",
      "292 torch.Size([32, 10])\n",
      "293 torch.Size([32, 10])\n",
      "294 torch.Size([32, 10])\n",
      "295 torch.Size([32, 10])\n",
      "296 torch.Size([32, 10])\n",
      "297 torch.Size([32, 10])\n",
      "298 torch.Size([32, 10])\n",
      "299 torch.Size([32, 10])\n",
      "300 torch.Size([32, 10])\n",
      "301 torch.Size([32, 10])\n",
      "302 torch.Size([32, 10])\n",
      "303 torch.Size([32, 10])\n",
      "304 torch.Size([32, 10])\n",
      "305 torch.Size([32, 10])\n",
      "306 torch.Size([32, 10])\n",
      "307 torch.Size([32, 10])\n",
      "308 torch.Size([32, 10])\n",
      "309 torch.Size([32, 10])\n",
      "310 torch.Size([32, 10])\n",
      "311 torch.Size([32, 10])\n",
      "312 torch.Size([32, 10])\n",
      "313 torch.Size([32, 10])\n",
      "314 torch.Size([32, 10])\n",
      "315 torch.Size([32, 10])\n",
      "316 torch.Size([32, 10])\n",
      "317 torch.Size([32, 10])\n",
      "318 torch.Size([32, 10])\n",
      "319 torch.Size([32, 10])\n",
      "320 torch.Size([32, 10])\n",
      "321 torch.Size([32, 10])\n",
      "322 torch.Size([32, 10])\n",
      "323 torch.Size([32, 10])\n",
      "324 torch.Size([32, 10])\n",
      "325 torch.Size([32, 10])\n",
      "326 torch.Size([32, 10])\n",
      "327 torch.Size([32, 10])\n",
      "328 torch.Size([32, 10])\n",
      "329 torch.Size([32, 10])\n",
      "330 torch.Size([32, 10])\n",
      "331 torch.Size([32, 10])\n",
      "332 torch.Size([32, 10])\n",
      "333 torch.Size([32, 10])\n",
      "334 torch.Size([32, 10])\n",
      "335 torch.Size([32, 10])\n",
      "336 torch.Size([32, 10])\n",
      "337 torch.Size([32, 10])\n",
      "338 torch.Size([32, 10])\n",
      "339 torch.Size([32, 10])\n",
      "340 torch.Size([32, 10])\n",
      "341 torch.Size([32, 10])\n",
      "342 torch.Size([32, 10])\n",
      "343 torch.Size([32, 10])\n",
      "344 torch.Size([32, 10])\n",
      "345 torch.Size([32, 10])\n",
      "346 torch.Size([32, 10])\n",
      "347 torch.Size([32, 10])\n",
      "348 torch.Size([32, 10])\n",
      "349 torch.Size([32, 10])\n",
      "350 torch.Size([32, 10])\n",
      "351 torch.Size([32, 10])\n",
      "352 torch.Size([32, 10])\n",
      "353 torch.Size([32, 10])\n",
      "354 torch.Size([32, 10])\n",
      "355 torch.Size([32, 10])\n",
      "356 torch.Size([32, 10])\n",
      "357 torch.Size([21, 10])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 1, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
