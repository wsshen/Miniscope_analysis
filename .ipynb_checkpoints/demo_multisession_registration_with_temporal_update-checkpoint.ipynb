{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multisession registration with CaImAn\n",
    "\n",
    "This notebook will help to demonstrate how to use CaImAn on movies recorded in multiple sessions. CaImAn has in-built functions that align movies from two or more sessions and try to recognize components that are imaged in some or all of these recordings.\n",
    "\n",
    "The basic function for this is `caiman.base.rois.register_ROIs()`. It takes two sets of spatial components and finds components present in both using an intersection over union metric and the Hungarian algorithm for optimal matching.\n",
    "`caiman.base.rois.register_multisession()` takes a list of spatial components, aligns sessions 1 and 2, keeps the union of the matched and unmatched components to register it with session 3 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 13:37:44.494400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from caiman.base.rois import register_multisession\n",
    "from caiman.utils import visualization\n",
    "from caiman.utils.utils import download_demo\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide an example file generated from data courtesy of Sue Ann Koay and David Tank (Princeton University). The file contains the spatial footprints derived from the CNMF analysis of the same FOV over six different days, as well as a template (correlation image) for each day. The `download_demo` command will automatically download the file and store it in your caiman_data folder the first time you run it. To use the demo in your own dataset you can set:\n",
    "\n",
    "```file_path = '/path/to/file'```\n",
    "\n",
    "or construct a list of spatial footprints and templates and use that to perform the registration as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multisession data (spatial components and mean intensity templates) (should be replaced by actual data)\n",
    "# file_path = download_demo('alignment.pickle')\n",
    "# infile = open(file_path,'rb')\n",
    "# data = pickle.load(infile)\n",
    "# infile.close()\n",
    "\n",
    "# spatial = data[0]\n",
    "# templates = data[1]\n",
    "# dims = templates[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir01 = '/home/watson/Documents/caiman_fromCluster/'\n",
    "dir02 = '/home/watson/Documents/caiman_fromCluster/49142/06072023/13_04_23/'\n",
    "\n",
    "data01 = 'cnmf_save.hdf5'\n",
    "data02 = 'cnmf_save_40_79.hdf5'\n",
    "\n",
    "temp01 = 'output_0_39.pickle'\n",
    "temp02 = 'output_40_79.pickle'\n",
    "\n",
    "import os\n",
    "tempFile01 = dir01 + os.sep + temp01\n",
    "tempFile02 = dir02 + os.sep + temp02\n",
    "\n",
    "dataFile01 = dir01 + os.sep + data01\n",
    "dataFile02 = dir02 + os.sep + data02\n",
    "\n",
    "with open(tempFile01, 'rb') as f:\n",
    "    template01 = pickle.load(f)\n",
    "with open(tempFile02, 'rb') as f:\n",
    "    template02 = pickle.load(f)\n",
    "\n",
    "from caiman.source_extraction.cnmf.cnmf import load_CNMF\n",
    "\n",
    "data1 = load_CNMF(dataFile01)\n",
    "data2 = load_CNMF(dataFile02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import caiman as cm\n",
    "# bord_px =0\n",
    "# fname_new = '/home/watson/Documents/caiman_fromCluster/49142/06072023/13_04_23/memmap_d1_600_d2_600_d3_1_order_C_frames_40000.mmap'\n",
    "# Yr,dims,T=cm.load_memmap(fname_new)\n",
    "# images = Yr.T.reshape((T,)+dims,order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial1=data1.estimates.A\n",
    "spatial2=data2.estimates.A\n",
    "spatial_np1 = spatial1.toarray()\n",
    "spatial_np2 = spatial2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1 = data1.estimates.idx_components\n",
    "idx_2 = data2.estimates.idx_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1.estimates.idx_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1.estimates.idx_components = np.append(idx_1,[37,54])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1.estimates.idx_componenets\n",
    "\n",
    "# data1.save(dir + os.sep + \"cnmf_save_manual.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# temp_file = \"cnmf_save_manual.hdf5\"\n",
    "# temp = load_CNMF(dir + os.sep + temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = [idx_1,idx_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_csc1 = csc_array(spatial_np1[:,idx_1],dtype=np.float64)\n",
    "spatial_csc2 = csc_array(spatial_np2[:,idx_2],dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_template = template01[0].shape\n",
    "templates = [template01[0],template02[0]]\n",
    "spatial = [spatial1,spatial2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm_list = [data1,data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size1 = data1.estimates.C.shape\n",
    "size2 = data2.estimates.C.shape\n",
    "tot_len = size1[1] + size2[1] \n",
    "length = [size1[1],size2[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from caiman.source_extraction import cnmf\n",
    "# from caiman.source_extraction.cnmf import params as params\n",
    "# min_SNR = 5            # adaptive way to set threshold on the transient size\n",
    "# r_values_min = 0.85\n",
    "# cnm=cnmf.CNMF(n_processes=1,dview=None,Ain=None,params=None)\n",
    "\n",
    "# cnm.params.set('quality', {'min_SNR': min_SNR,\n",
    "#                            'rval_thr': r_values_min,\n",
    "#                            'use_cnn': False})\n",
    "\n",
    "# data1.estimates.evaluate_components(images, cnm.params, dview=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_1_new = data1.estimates.idx_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_colors ={\n",
    "  \"manual_color\": \"Yellow\",\n",
    "  \"approved_color\": \"Green\",\n",
    "  \"refused_color\": \"Red\"\n",
    "}\n",
    "print(bin_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_colors.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial1=data1.estimates.A\n",
    "# spatial_np1 = spatial1.toarray()\n",
    "# spatial_csc1 = csc_array(spatial_np1[:,idx_1],dtype=np.float64)\n",
    "\n",
    "# visualization.plot_contours(spatial_csc1, templates[0],colors='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_new =  '/home/watson/Documents/caiman_fromCluster/49142/06072023/13_04_23/memmap_d1_600_d2_600_d3_1_order_C_frames_39245.mmap'\n",
    "import caiman as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yr_data,dims_memmap,T_memmap=cm.load_memmap(fname_new)\n",
    "images_memmap = Yr_data.T.reshape((T_memmap,)+dims_memmap,order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_contours(spatial1[:,9], templates[0],colors='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_contours(spatial2, templates[1],colors='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_2 = data2.estimates.A\n",
    "b_2 = data2.estimates.b\n",
    "C_2 = data2.estimates.C\n",
    "f_2 = data2.estimates.f\n",
    "bl_2 = data2.estimates.bl\n",
    "c1_2 = data2.estimates.c1\n",
    "g_2 = data2.estimates.g\n",
    "sn_2 = data2.estimates.sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_added = np.append(spatial_np1, spatial_np2[:,[20]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_added.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from caiman.source_extraction.cnmf.initialization import downscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def findEstimatesB(savedEst,imgs):\n",
    "#     AC = savedEst.A.dot(savedEst.C)\n",
    "#     if savedEst.W is not None:\n",
    "#         ssub_B = int(round(np.sqrt(np.prod(dims) / savedEst.W.shape[0])))\n",
    "#         B = imgs.reshape((-1, np.prod(dims)), order='F').T - AC\n",
    "#         if ssub_B == 1:\n",
    "#             B = savedEst.b0[:, None] + savedEst.W.dot(B - savedEst.b0[:, None])\n",
    "#         else:\n",
    "#             WB = savedEst.W.dot(downscale(B.reshape(dims + (B.shape[-1],), order='F'),\n",
    "#                           (ssub_B, ssub_B, 1)).reshape((-1, B.shape[-1]), order='F'))\n",
    "#             Wb0 = savedEst.W.dot(downscale(savedEst.b0.reshape(dims, order='F'),\n",
    "#                           (ssub_B, ssub_B)).reshape((-1, 1), order='F'))\n",
    "#             B = savedEst.b0.flatten('F')[:, None] + (np.repeat(np.repeat((WB - Wb0).reshape(((dims[0] - 1) // ssub_B + 1, (dims[1] - 1) // ssub_B + 1, -1), order='F'),\n",
    "#                                  ssub_B, 0), ssub_B, 1)[:dims[0], :dims[1]].reshape((-1, B.shape[-1]), order='F'))\n",
    "#         B = B.reshape(dims + (-1,), order='F').transpose([2, 0, 1])\n",
    "#     elif savedEst.b is not None and savedEst.f is not None:\n",
    "#         B = savedEst.b.dot(savedEst.f)\n",
    "#         if 'matrix' in str(type(B)):\n",
    "#             B = B.toarray()\n",
    "#         B = B.reshape(dims + (-1,), order='F').transpose([2, 0, 1])\n",
    "#     else:\n",
    "#         B = np.zeros_like(Y_rec)\n",
    "\n",
    "# savedEst = data1.estimates\n",
    "# B = findEstimatesB(savedEst,images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def HALS4activity(Yr, A, b, iters=100):\n",
    "#     if b is not None:\n",
    "#         Ab =  np.c_[A, b]\n",
    "#     else:\n",
    "#         Ab = A\n",
    "#     U = Ab.T.dot(Yr)\n",
    "#     V = Ab.T.dot(Ab) + np.finfo(Ab.dtype).eps\n",
    "#     Cf = U/V.diagonal()[:,None]\n",
    "#     for _ in range(iters):\n",
    "#         for m in range(len(U)):  # neurons and background\n",
    "#             Cf[m] = np.clip(Cf[m] + (U[m] - V[m].dot(Cf)) / V[m, m], 0, np.inf)\n",
    "#     return Cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cf = HALS4activity(Yr,A,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dir02+os.sep+'A_added.pickle','wb') as f:\n",
    "#     pickle.dump(A_added,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir01+os.sep+'cf.pickle','rb') as f:\n",
    "    C_init = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging \n",
    "# logging.basicConfig(format=\n",
    "#                           \"%(relativeCreated)12d [%(filename)s:%(funcName)20s():%(lineno)s] [%(process)d] %(message)s\",\n",
    "#                     # filename=\"/tmp/caiman.log\",\n",
    "#                     level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caiman.source_extraction.cnmf.temporal import update_temporal_components\n",
    "# C_new, A_new, b_new, f_new, S_new, bl_new, c1_new, sn_new, g_new, YrA_new, lam_new\n",
    "C_new, A_new, b_new, f_new, S_new, bl_new, c1_new, sn_new, g_new, YrA_new, lam_new = update_temporal_components(Yr_data, A_2, b_2, C_init,fin=None,bl=bl_2,c1=c1_2,g=g_2,sn=sn_2,p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caiman.source_extraction.cnmf.utilities import detrend_df_f\n",
    "F_df_new = detrend_df_f(A_2,b_new,C_new,f_new,YrA = YrA_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial_np1_new =A_new.toarray()\n",
    "# spatial_csc1_new = csc_array(spatial_np1_new[:,idx_2[:-2]],dtype=np.float64)\n",
    "# visualization.plot_contours(spatial1[:,13], templates[0],colors='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_contours(spatial2[:,19], templates[1],colors='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_contours(A_new[:,19], templates[1],colors='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "C_original = data2.estimates.C\n",
    "plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(C_2[17])\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(F_df_new[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(C_original,F_df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_trace(denoised_trace, \n",
    "                 raw_trace, \n",
    "                 offset_method = 'floor', \n",
    "                 sn_method='logmexp', \n",
    "                 range_ff=[0.25, 0.5]):\n",
    "    \"\"\"\n",
    "    \"Z-score\" calcium traces based on a calculation of noise from\n",
    "    power spectral density high frequency.\n",
    "\n",
    "    Inputs:\n",
    "        denoised_trace: from estimates.C\n",
    "        raw_trace: from estimates.C + estimates.YrA\n",
    "        offset_method: offset to use when shifting the trace (default: 'floor')\n",
    "            'floor': minimum of the trace so new min will be zero\n",
    "            'mean': mean of the raw/denoised trace as the zeroing point\n",
    "            'median': median of raw/denoised trace\n",
    "            'none': no offset just normalize\n",
    "        sn_method: how are psd central values caluclated \n",
    "            mean\n",
    "            median' or 'logmexp')\n",
    "        range_ff: 2-elt array-like range of frequencies (input for GetSn) (default [0.25, 0.5])\n",
    "\n",
    "    Returns \n",
    "        z_denoised: same shape as denoised_trace\n",
    "        z_raw: same shape as raw trace\n",
    "        trace_noise: noise level from z_raw\n",
    "        \n",
    "    Adapted from code by Zach Barry.\n",
    "    \"\"\"\n",
    "    noise = GetSn(raw_trace, range_ff=range_ff, method=sn_method)  #import this from caiman\n",
    "\n",
    "    if offset_method == 'floor':\n",
    "        raw_offset = np.min(raw_trace)\n",
    "        denoised_offset = np.min(denoised_trace)\n",
    "    elif offset_method == 'mean':\n",
    "        raw_offset = np.mean(raw_trace)\n",
    "        denoised_offset = np.mean(denoised_trace)\n",
    "    elif offset_method == 'median':\n",
    "        raw_offset = np.median(raw_trace)\n",
    "        denoised_offset = np.median(denoised_trace)\n",
    "    elif offset_method == 'none':\n",
    "        raw_offset = 0\n",
    "        denoised_offset = 0\n",
    "    else:\n",
    "        raise ValueError(\"offset_method should be floor, mean, median, or none.\")\n",
    "           \n",
    "    z_raw = (raw_trace - raw_offset) / noise\n",
    "    z_denoised = (denoised_trace - denoised_offset)/ noise\n",
    "        \n",
    "    return z_denoised, z_raw, noise\n",
    "\n",
    "def zscore_traces(cnm_c, \n",
    "                  cnm_yra, \n",
    "                  offset_method = 'floor', \n",
    "                  sn_method = 'logmexp', \n",
    "                  range_ff=[0.25, 0.5]):\n",
    "    \"\"\"\n",
    "    apply zscore_trace to all traces in estimates\n",
    "    \n",
    "    inputs:\n",
    "        cnm_c: C array of denoised traces from cnm.estimates\n",
    "        cnm_yra: YrA array of residuals from cnm.estimate\n",
    "        offset_method: floor/mean/median (see zscore_trace)\n",
    "        sn_method: mean/median/logmexp (see zscore_trace)\n",
    "        range_ff: frequency range for GetSn\n",
    "    \n",
    "    outputs:\n",
    "        denoised_z_traces\n",
    "        raw_z_traces\n",
    "        noise_all\n",
    "    \"\"\"\n",
    "    raw_traces = cnm_c + cnm_yra  # raw_trace[i] = c[i] + yra[i]\n",
    "    raw_z_traces = []\n",
    "    denoised_z_traces = []\n",
    "    noise_all = []\n",
    "    for ind, raw_trace in enumerate(raw_traces):\n",
    "        denoised_trace = cnm_c[ind,:]\n",
    "        z_denoised, z_raw, noise = zscore_trace(denoised_trace,\n",
    "                                                raw_trace, \n",
    "                                                offset_method=offset_method, \n",
    "                                                sn_method = sn_method,\n",
    "                                                range_ff=range_ff)\n",
    "        \n",
    "        denoised_z_traces.append(z_denoised)\n",
    "        raw_z_traces.append(z_raw)\n",
    "        noise_all.append(noise)\n",
    "        \n",
    "    denoised_z_traces = np.array(denoised_z_traces)\n",
    "    raw_z_traces = np.array(raw_z_traces)\n",
    "    noise_all = np.array(noise_all)\n",
    "    \n",
    "    return denoised_z_traces, raw_z_traces, noise_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caiman.source_extraction.cnmf.deconvolution import GetSn\n",
    "denoised_traces = data1.estimates.C\n",
    "residuals = data1.estimates.YrA\n",
    "# z_denoised_floor, z_raw_floor, _ = zscore_trace(denoised_traces[13,:], raw_traces[13,:], offset_method='floor', sn_method='logmexp')\n",
    "denoised_z_traces, raw_z_traces, noise_all = zscore_traces(denoised_traces, residuals, offset_method='floor')\n",
    "\n",
    "denoised_traces_new = F_df_new\n",
    "residuals_new = YrA_new\n",
    "denoised_z_traces_new, raw_z_traces_new, _ = zscore_traces(denoised_traces_new, residuals_new, offset_method='floor', sn_method='logmexp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `register_multisession()`\n",
    "\n",
    "The function `register_multisession()` requires 3 arguments:\n",
    "- `A`: A list of ndarrays or scipy.sparse.csc matrices with (# pixels X # component ROIs) for each session\n",
    "- `dims`: Dimensions of the FOV, needed to restore spatial components to a 2D image\n",
    "- `templates`: List of ndarray matrices of size `dims`, template image of each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_new = [spatial1,A_new]\n",
    "templates_new = [template01[0],template01[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_union, assignments, matchings = register_multisession(A=spatial_new, dims=dims_template, templates=templates_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns 3 variables for further analysis:\n",
    "- `spatial_union`: csc_matrix (# pixels X # total distinct components), the union of all ROIs across all sessions aligned to the FOV of the last session.\n",
    "- `assignments`: ndarray (# total distinct components X # sessions). `assignments[i,j]=k` means that component `k` from session `j` has been identified as component `i` from the union of all components, otherwise it takes a `NaN` value. Note that for each `i` there is at least one session index `j` where `assignments[i,j]!=NaN`.\n",
    "- `matchings`: list of (# sessions) lists. Saves `spatial_union` indices of individual components in each session. `matchings[j][k] = i` means that component `k` from session `j` is represented by component `i` in the union of all components `spatial_union`. In other words `assignments[matchings[j][k], j] = j`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-alignment screening\n",
    "\n",
    "The three outputs can be used to filter components in various ways. For example we can find the components that were active in at least a given a number of sessions. For more examples, check [this script](https://github.com/flatironinstitute/CaImAn/blob/master/use_cases/eLife_scripts/figure_9/Figure_9_alignment.py) that reproduces the results of [Figure 9, as presented in our eLife paper](https://elifesciences.org/articles/38173#fig9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter components by number of sessions the component could be found\n",
    "\n",
    "n_reg = 1\n",
    "# Use number of non-NaNs in each row to filter out components that were not registered in enough sessions\n",
    "assignments_filtered = np.array(np.nan_to_num(assignments[np.sum(~np.isnan(assignments), axis=1) >= n_reg]), dtype=int);\n",
    "\n",
    "# Use filtered indices to select the corresponding spatial components\n",
    "spatial_filtered = spatial[0][:, assignments_filtered[:, 0]]\n",
    "\n",
    "# Plot spatial components of the selected components on the template of the last session\n",
    "visualization.plot_contours(spatial_filtered, templates[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_combined = []\n",
    "for i in assignments:\n",
    "    if (i[0] in idx_1 and (not np.isnan(i[1]))) or (i[1] in idx_2 and (not np.isnan(i[0]))):\n",
    "        assignments_combined.append(i)\n",
    "        \n",
    "# assignments_combined.append(np.array([0,47]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_combined_np = np.array(assignments_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[int(i) for i in assignments_combined_np[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_combined_np[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining data of components over multiple sessions\n",
    "\n",
    "Now that all sessions are aligned and we have a list of re-registered neurons, we can use `assignments` and `matchings` to collect traces from neurons over different sessions.\n",
    "\n",
    "As an exercise, we can collect the traces of all neurons that were registered in all sessions. We already gathered the indices of these neurons in the previous cell in `assignments_filtered`. Assuming that traces of each session are saved in their own `CNMF` object collected in a list, we can iterate through `assignments_filtered` and use these indices to find the re-registered neurons in every session.\n",
    "\n",
    "Note: This notebook does not include the traces of the extracted neurons, only their spatial components. As such the loop below will produce an error. However, it demonstrates how to use the results of the registration to in your own analysis to extract the traces of the same neurons across different sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traces_ = np.zeros((assignments_combined_np.shape[0],) + (tot_len,), dtype=np.ndarray)\n",
    "# print(traces_.shape)\n",
    "# for i in range(traces_.shape[0]):\n",
    "#     for j in range(assignments_combined_np.shape[1]):\n",
    "#         print(assignments_combined_np[i,j])\n",
    "#         traces_[i,sum(length[0:j]):sum(length[0:j+1])] = cnm_list[j].estimates.C[[int(assignments_combined_np[i,j])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traces_ = np.zeros((assignments_combined_np.shape[0],) + (length[1],), dtype=np.ndarray)\n",
    "# print(traces_.shape)\n",
    "# for i in range(traces_.shape[0]):\n",
    "#     j=1\n",
    "#     print(assignments_combined_np[i,j])\n",
    "#     traces_[i,:] = cnm_list[j].estimates.C[[int(assignments_combined_np[i,j])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## original segment in the demo \n",
    "# traces = np.zeros(assignments_filtered.shape, dtype=np.ndarray)\n",
    "# for i in range(traces.shape[0]):\n",
    "#     for j in range(traces.shape[1]):\n",
    "        \n",
    "#         traces[i,j] = cnm_list[j].estimates.C[int(assignments_filtered[i,j])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dir02+'traces_registered.pickle', 'wb') as f:\n",
    "#     pickle.dump(traces_, f)\n",
    "# with open(dir02+'idx_registered.pickle', 'wb') as f:\n",
    "#     pickle.dump(assignments_combined_np[:,0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir03 = '/media/watson/UbuntuHDD/feng_Xin/Xin/Miniscope/4914202252023/06072023/12_52_03/'\n",
    "# with open(dir03+os.sep+'idx_registered.pickle', 'rb') as f:\n",
    "#     aaa = pickle.load(f)\n",
    "# print(aaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the array `traces`, where element `traces[i,j] = k` is the temporal component of neuron `i` at session `j`. This can be performed with `F_dff` data or `S` spikes as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
